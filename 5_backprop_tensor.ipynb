{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZXfH7ZWqRuHEQoCufN2+0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bubuloMallone/NeuralNetworksLM/blob/main/5_backprop_tensor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding the Backpropagation Algorithm\n",
        "\n",
        "The aim of this notebook is to implement from scratch the back propagation alogorithm, this time at the tensors level (like the PyTorch .backprop() method) to understand how it works under the hood. This helps also understanding how to use it optimally in further training."
      ],
      "metadata": {
        "id": "xERYumr0YUVf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28GK1_DdYTTf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read all the words dataset\n",
        "\n",
        "!wget https://raw.githubusercontent.com/bubuloMallone/NeuralNetworksLM/refs/heads/main/datasets/names.txt\n",
        "\n",
        "words = open('names.txt', 'r').read().splitlines()\n",
        "\n",
        "words[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nk__1tjfbZk8",
        "outputId": "81fa0596-0469-4e10-d898-45a1c601f840"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-15 09:33:09--  https://raw.githubusercontent.com/bubuloMallone/NeuralNetworksLM/refs/heads/main/datasets/names.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228145 (223K) [text/plain]\n",
            "Saving to: ‘names.txt’\n",
            "\n",
            "names.txt           100%[===================>] 222.80K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-07-15 09:33:09 (6.19 MB/s) - ‘names.txt’ saved [228145/228145]\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['emma',\n",
              " 'olivia',\n",
              " 'ava',\n",
              " 'isabella',\n",
              " 'sophia',\n",
              " 'charlotte',\n",
              " 'mia',\n",
              " 'amelia',\n",
              " 'harper',\n",
              " 'evelyn']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build the vocabulary of characters and mappings to/from integers\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s,i in stoi.items()}\n",
        "vocab_size = len(itos)\n",
        "print(itos)\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Daklhel6bk3g",
        "outputId": "32956c2c-37d1-4250-a12a-8840a345434e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
            "27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build the dataset\n",
        "\n",
        "# define the context length: how many char we consider to predict the next one\n",
        "block_size = 3\n",
        "\n",
        "def build_dataset(words):\n",
        "\n",
        "  X, Y = [], []\n",
        "  for word in words:\n",
        "    context = [0] * block_size\n",
        "    for ch in word + '.':\n",
        "      idx = stoi[ch]\n",
        "      X.append(context)\n",
        "      Y.append(idx)\n",
        "      # print(''.join(itos[i] for i in context), '--->', itos[idx])\n",
        "      context = context[1:] + [idx]\n",
        "  X = torch.tensor(X)\n",
        "  Y = torch.tensor(Y)\n",
        "  print('Data:', X.shape, X.dtype)\n",
        "  print('Labels:', Y.shape, Y.dtype)\n",
        "  num_samples = X.shape[0]\n",
        "\n",
        "  return X, Y, num_samples\n",
        "\n",
        "random.seed(42)\n",
        "random.shuffle(words)\n",
        "n1 = int(0.8*len(words))\n",
        "n2 = int(0.9*len(words))\n",
        "\n",
        "Xtr, Ytr, num_samples_tr = build_dataset(words[:n1])\n",
        "Xval, Yval, num_samples_val = build_dataset(words[n1:n2])\n",
        "Xte, Yte, num_samples_te = build_dataset(words[n2:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgVZSOq_cIn5",
        "outputId": "37c20b7b-b4b8-457d-e324-7c07bd3893fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data: torch.Size([182625, 3]) torch.int64\n",
            "Labels: torch.Size([182625]) torch.int64\n",
            "Data: torch.Size([22655, 3]) torch.int64\n",
            "Labels: torch.Size([22655]) torch.int64\n",
            "Data: torch.Size([22866, 3]) torch.int64\n",
            "Labels: torch.Size([22866]) torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First let us define a utility function to compare the manual gradients computed to PyTorch gradients."
      ],
      "metadata": {
        "id": "t2PReI9KcpFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# utility function to compare gradients to PyTorch gradients\n",
        "def cmp(s, dt, t):\n",
        "  ex = torch.all(dt == t.grad).item()\n",
        "  app = torch.allclose(dt, t.grad)\n",
        "  maxdiff = (dt - t.grad).abs().max().item()\n",
        "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')\n"
      ],
      "metadata": {
        "id": "eLsIHs6Dcitu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let us initialize the neular network (MLP) as usual"
      ],
      "metadata": {
        "id": "H1idjqf1eD9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP architecture\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "\n",
        "emb_dim = 10  # the dimensionality of the character embedding vectors\n",
        "hidden_dim = 200  # the number of neurons in the hidden layer of the MLP\n",
        "\n",
        "# embedding\n",
        "C = torch.randn((vocab_size, emb_dim))\n",
        "# Layer 1\n",
        "W1 = torch.randn((block_size * emb_dim, hidden_dim), generator=g) * (5/3)/((block_size * emb_dim)**0.5) # Kaiming init for tanh! * 0.2  # to avoid dead neurons\n",
        "b1 = torch.randn(hidden_dim, generator=g) * 0.1  # keep b1 just for fun (check the grads) even though it is actually useless\n",
        "# Layer 2\n",
        "W2 = torch.randn((hidden_dim, vocab_size), generator=g) * 0.1  # to keep low initial logits\n",
        "b2 = torch.randn(vocab_size, generator=g) * 0.1   # to keep low initial logits\n",
        "# BatchNorm parameters\n",
        "bn_gain = torch.randn((1, hidden_dim)) * 0.1 + 1.0\n",
        "bn_bias = torch.randn((1, hidden_dim)) * 0.1\n",
        "\n",
        "# Note: many of these parameters are initialized in non-standard ways because the correct\n",
        "# initializations sometimes might mask some incorrect implementations of the backward pass\n",
        "\n",
        "\n",
        "parameters = [C, W1, b1, W2, b2, bn_gain, bn_bias]\n",
        "\n",
        "# require gradients\n",
        "for p in parameters:\n",
        "  p.requires_grad = True\n",
        "\n",
        "tot_parameters = sum(p.nelement() for p in parameters)\n",
        "print(f'Total number of parameters: {tot_parameters}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFMQtzovd97-",
        "outputId": "e339508d-7d30-4e8b-91d6-db88c200a119"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters: 12297\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "# minibatch construct\n",
        "idxs = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)  # (num_samples,) --> (batch_size,)\n",
        "Xb, Yb = Xtr[idxs], Ytr[idxs]  # (batch_size, block_size) and (batch_size,)"
      ],
      "metadata": {
        "id": "Ex8qQLV7tP_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the forward pass in small steps sothat is possible to backward at every step."
      ],
      "metadata": {
        "id": "l3_aiKiptcZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward pass\n",
        "# embedd characters into vectors\n",
        "emb = C[Xb]   # (batch_size, block_size, emb_dim)\n",
        "# concatenate the vectors\n",
        "emb_cat = emb.view(emb.shape[0], -1)  # (batch_size, block_size * emb_dim)\n",
        "# linear layer 1\n",
        "h_preact = emb_cat @ W1 + b1   # (batch_size, hidden_dim)\n",
        "# batch norm layer\n",
        "bn_mean = h_preact.mean(0, keepdim=True)  # (1, hidden_dim)\n",
        "bn_diff = h_preact - bn_mean_i  # (batch_size, hidden_dim)\n",
        "bn_diff2 = bn_diff ** 2\n",
        "bn_var = bn_diff2.mean(0, keepdim=True)  # (1, hidden_dim)\n",
        "bn_std_inv = (bn_var + 1e-5) ** -0.5  # (1, hidden_dim)\n",
        "bn_raw = bn_diff * bn_std_inv  # (batch_size, hidden_dim)\n",
        "# normalize the batch to unit gaussian and then offset/scale it according to learned bn_bias/gain (might add +eps small to std)\n",
        "h_preact_norm = bn_raw * bn_gain + bn_bias  # (batch_size, hidden_dim)\n",
        "# non-linear activation\n",
        "h = torch.tanh(h_preact)   # (batch_size, hidden_dim)\n",
        "# linear layer 2\n",
        "logits = h @ W2 + b2    # output (batch_size, alphabet_size)\n",
        "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
        "logit_max = logits.max(1, keepdim=True).values  # (batch_size, 1)\n",
        "norm_logits = logits - logit_max  # subtract max for numerical stability (batch_size, alphabet_size)\n",
        "counts = norm_logits.exp() # (batch_size, alphabet_size)\n",
        "counts_sum = counts.sum(1, keepdim=True) # (batch_size, 1)\n",
        "counts_sum_inv = counts_sum ** -1  # (batch_size, 1)\n",
        "probs = counts * counts_sum_inv  # (batch_size, alphabet_size)\n",
        "logprobs = probs.log()\n",
        "loss = -logprobs[range(batch_size), Yb].mean()\n",
        "\n",
        "\n",
        "# PyTorch backward pass\n",
        "for p in parameters:\n",
        "  p.grad = None\n",
        "\n",
        "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
        "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
        "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
        "         embcat, emb]:\n",
        "  t.retain_grad()\n",
        "\n",
        "loss.backward()"
      ],
      "metadata": {
        "id": "cKY_Kn3kg_ms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Backpropagation implementation 1\n",
        "\n",
        "Manually implement the backpropagation algorithm to propagate through all the variables defined in the forward pass, one by one.\n"
      ],
      "metadata": {
        "id": "FNfB9HFFl7MJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Approach 1: backprop through the whole thing manually, one by one,\n",
        "# through all the variables defined in the forward pass above.\n",
        "\n",
        "# compute derivatives according to the chain rule dLoss/dparams\n",
        "\n",
        "# dLoss/dLogprobs = d[- 1/batch_size *(sum_i x_i)]/dx_j = -1/batch_size * 1_ij\n",
        "dlogprobs = torch.zeros_like(logprobs)\n",
        "dlogprobs[range(batch_size), Yb] = -1.0/batch_size\n",
        "cmp('logprobs', dlogprobs, logprobs)\n",
        "\n",
        "# dLoss/dProbs = dLoss/dLogprobs * dLogprobs/dProbs = dlogprobs * 1/probs\n",
        "dprobs = (1.0/probs) * dlogprobs\n",
        "cmp('probs', dprobs, probs)\n",
        "\n",
        "# dLoss/dCounts_sum_inv = dLoss/dProbs * dProbs/dCounts_sum_inv\n",
        "# probs = counts * counts_sum_inv [(batch_size, alphabet_size) * (batch_size, 1)]\n",
        "# c = a * b with a[3x3], b[3,1] -->\n",
        "# c : [[a11*b1 a12*b1 a13*b1]\n",
        "#      [a21*b2 a22*b2 a23*b2]\n",
        "#      [a31*b3 a32*b3 a33*b3]]\n",
        "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\n",
        "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
        "dcounts = counts_sum_inv * dprobs\n",
        "cmp('counts', dcounts, counts)\n",
        "\n",
        "dcounts_sum = (-counts_sum ** -2) * dcounts_sum_inv\n",
        "cmp('counts_sum', dcounts_sum, counts_sum)\n",
        "\n",
        "# dLoss/dCounts = dLoss/dCounts_sum * dCounts_sum/dCounts\n",
        "# counts [(batch_size, alphabet_size)] --> counts_sum [(batch_size, 1)]\n",
        "# [c11 c12 c13] -->  s1 = c11 + c12 + c13\n",
        "# [c21 c22 c23] -->  s2 = c21 + c22 + c23\n",
        "# [c31 c32 c33] -->  s3 = c31 + c32 + c33\n",
        "dcounts = torch.ones_like(counts) * dcounts_sum\n",
        "\n",
        "\n",
        "# cmp('counts', dcounts, counts)\n",
        "# cmp('norm_logits', dnorm_logits, norm_logits)\n",
        "# cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
        "# cmp('logits', dlogits, logits)\n",
        "# cmp('h', dh, h)\n",
        "# cmp('W2', dW2, W2)\n",
        "# cmp('b2', db2, b2)\n",
        "# cmp('hpreact', dhpreact, hpreact)\n",
        "# cmp('bngain', dbngain, bngain)\n",
        "# cmp('bnbias', dbnbias, bnbias)\n",
        "# cmp('bnraw', dbnraw, bnraw)\n",
        "# cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
        "# cmp('bnvar', dbnvar, bnvar)\n",
        "# cmp('bndiff2', dbndiff2, bndiff2)\n",
        "# cmp('bndiff', dbndiff, bndiff)\n",
        "# cmp('bnmeani', dbnmeani, bnmeani)\n",
        "# cmp('hprebn', dhprebn, hprebn)\n",
        "# cmp('embcat', dembcat, embcat)\n",
        "# cmp('W1', dW1, W1)\n",
        "# cmp('b1', db1, b1)\n",
        "# cmp('emb', demb, emb)\n",
        "# cmp('C', dC, C)"
      ],
      "metadata": {
        "id": "_-2hn9q6lh2x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}