# NeuralNetworksEX

This repository contains a collection of Python implementations of fundamental deep learning architectures and algorithms.  
The goal of this project is **educational**: to build neural networks and training routines both from scratch and   relying on high-level frameworks such as PyTorch, in order to gain an in-depth understanding of how modern deep learning methods work under the hood. 

---

## ðŸ“‚ Repository Contents

1. **Basic Neural Networks and Backpropagation**
   - A simple feedforward neural network implemented from first principles.
   - Includes a manual implementation of the **backpropagation algorithm** for training.
   - Demonstrates forward and backward passes with gradient updates.

2. **Bigram Language Model (Basic LLM Architecture)**
   - A minimal character-level bigram language model.
   - Captures simple statistical dependencies between characters.
   - Provides an introduction to how large language models (LLMs) are built up from simple concepts.

3. **Extended Bigram Model**
   - A slightly more advanced variant of the basic bigram model.
   - Improves on training and evaluation, showing step-by-step how LLM-style architectures evolve.

4. **Improved Neural Network with Modern Training Practices**
   - Implementation of a network with better **weight initialization** strategies.
   - Incorporates **batch normalization** for stable and efficient training.
   - Highlights the importance of initialization and normalization in deep learning.

5. **Tensor-Level Backpropagation**
   - Backpropagation implemented fully at the **tensor level**, without relying on automatic differentiation libraries.
   - Shows how matrix calculus and vectorized operations can generalize backpropagation for more complex architectures.

---

## ðŸš€ Goals

- Build a deep understanding of how neural networks learn.
- Explore the mathematical and algorithmic foundations of backpropagation.
- Connect toy models (bigram LLMs) to modern large-scale architectures.
- Learn practical best practices such as initialization and normalization.

---
